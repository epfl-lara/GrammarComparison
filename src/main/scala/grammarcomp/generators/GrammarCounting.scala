package grammarcomp

package generators

import grammar._
import grammar.CFGrammar._
import grammar.utils._
import scala.collection.mutable.ListBuffer

object RandomAccessGeneratorUtil {

  /**
   * TODO: this can be more efficiently implemented using a
   * backward shortest path
   */
  def basecaseOfNonterminals[T](g: Grammar[T]): Map[Nonterminal, Rule[T]] = {
    var basecase = Map[Nonterminal, Rule[T]]()
    var continue = true
    while (continue) {
      continue = false
      //in every step find a non-terminal that has a productions containing only 
      //non-terminals for which the base case is known      
      g.nontermToRules.foreach {
        case (nt, rules) if (!basecase.contains(nt)) =>
          val baseRule = rules.find(_.rightSide.forall {
            case t: Terminal[T] => true
            case nt: Nonterminal =>
              basecase.contains(nt)
          })
          if (baseRule.isDefined) {
            basecase += (nt -> baseRule.get)
            continue = true
          }
        case _ =>
          ; //skip this non-terminal
      }
    }
    basecase
  }
  
  /**
   * Requires the grammar to not
   * have unproductive symbols
   */
  def minWords[T](g: Grammar[T]) = {
    var minwordMap = Map[Symbol[T], Word[T]]()
    var visited = Set[Symbol[T]]() // this will only have non-terminals

    def dfsRec(nt: Nonterminal): Word[T] = {      
      //ignore everything that is visited but does not have a minwordMap
      val rhsMinwords = g.nontermToRules(nt).collect {
        case Rule(_, rhs) if rhs.forall(s => !visited(s) ||
          minwordMap.contains(s)) =>            
          rhs.flatMap {
            case t: Terminal[T] =>
              List(t)
            case nt: Nonterminal if visited(nt) =>
              minwordMap(nt)
            case nt: Nonterminal =>
              visited += nt
              dfsRec(nt)
          }        
      }
      if (rhsMinwords.isEmpty)
        throw new IllegalStateException("Grammar has unproductive symbols: " + nt)
      else {
        val minNTword = rhsMinwords.minBy(_.size)        
        minwordMap += (nt -> minNTword)
        minNTword
      }
    }
    var unexploredNts = g.nonTerminals 
    while(!unexploredNts.isEmpty) {
      val nt = unexploredNts.head
      visited += nt
      dfsRec(nt)
      unexploredNts = unexploredNts.filterNot(visited.contains _)
    }    
    minwordMap
  }

  
  val one = BigInt(1)
  val zero = BigInt(0)
  def wordCountOfNonterminals[T](g: Grammar[T]): Map[Nonterminal, BigInt] = {
    var wordCount = Map[Nonterminal, BigInt]()
    var continue = true
    while (continue) {
      continue = false
      //in every step find a non-terminal that has only productions containing  
      //non-terminals for which a word count is known.
      //The wordcount is computed as the sum of the word count of all of its productions
      g.nontermToRules.foreach {
        case (nt, rules) if (!wordCount.contains(nt)) =>
          if (rules.forall(_.rightSide.forall {
            case t: Terminal[T] => true
            case rnt: Nonterminal =>
              wordCount.contains(rnt)
          })) {

            wordCount += (nt -> rules.map(_.rightSide.map {
              case t: Terminal[T] => one
              case nt: Nonterminal => wordCount(nt)
            }.product).sum)
            continue = true
          }
        case _ =>
          ; //skip this non-terminal
      }
    }
    wordCount
  }

  /**
   * Number of words that could generated by the non-terminal of the given 'size'.
   * Note that this number is always bounded if epsilons are also considered as terminals.
   * Note: in this class 'Int' data types are for word sizes and 'BigInt' for number of words.
   */
  class WordCounter[T](g: Grammar[T], size: Int) {
    val nontermToIndex = g.nontermsInPostOrder.zipWithIndex.toMap
    val indexToNonterms = nontermToIndex.map { case (nt, i) => (i, nt) }.toMap

    val N = g.nonTerminals.size
    private var wordCount = Array.fill[BigInt](N, size)(zero) //this will fill everything with zero

    def symCount(nt: Nonterminal, m: Int) = {
      if (m <= 0) zero
      else
        wordCount(nontermToIndex(nt))(m - 1)
    }

    //only for sizes > cacheBeginSize we start caching
    //val cacheBeginSize = 1
    var splitsCache = Map[(Rule[T], Int), List[(Int, BigInt)]]()
    def possibleSplitsForRule(rl: Rule[T], m: Int): List[(Int, BigInt)] = {
      //println("Rule: "+rl)
      //assume that the right-side has at most two non-terminals
      //TODO: extend this later to multiple ones.      
      val nonterms = nontermsInRightSide(rl)
      nonterms match {
        case List() =>
          //no non-terms implies only one word is possible and it should be equal to the size of 'rhs'
          val rhsSize = rl.rightSide.size
          if (m == rhsSize)
            List((m, one))
          else if (rhsSize == 0 && m == 1) //handle the epsilon special case 
            List((m, one))
          else List()

        case List(nt) =>
          //here, there are 'rl.rightSide.size - 1' terminals and only one non-terminal
          val ntsize = m - (rl.rightSide.size - 1)
          val sc = symCount(nt, ntsize)
          //          /println("Nonterm, ntindex, m, ntsize: "+(nt, nontermToIndex(nt), m, ntsize)+" sc: "+sc)
          if (sc > 0)
            List((ntsize, sc))
          else List()

        case List(nt1, nt2) =>
          //here, there are two nonterminals 'rl.rightSide.size - 2' terminals
          val ntsize = m - (rl.rightSide.size - 2)

          def computeSplits = {
            var splits = List[(Int, BigInt)]()
            for (j <- 1 to ntsize - 1) {
              val sc1 = symCount(nt1, j)
              if (sc1 > 0) {
                val sc2 = symCount(nt2, ntsize - j)
                if (sc2 > 0)
                  splits = (j, (sc1 * sc2)) +: splits
              }
            }
            //sort the splits based on 'bounds'
            splits.sortBy(_._2)
          }

          //if (ntsize >= cacheBeginSize) {
          if (splitsCache.contains((rl, ntsize)))
            splitsCache((rl, ntsize))
          else {
            val splits = computeSplits
            splitsCache += ((rl, ntsize) -> splits)
            splits
          }
        /*} else
            computeSplits*/
      }
    }

    def ruleCountForSize(rl: Rule[T], m: Int): BigInt = {
      val cnt = possibleSplitsForRule(rl, m).map(_._2).sum
      //      val list =  List(Nonterminal("expression1"), Nonterminal("expressionErr2"), 
      //          Nonterminal("N-3067"),Nonterminal("N-3068"),Nonterminal("N-1227"),Nonterminal("N-1228"))      
      //      if(m == 50 && list.contains(rl.leftSide)){
      //        //if(rl.rightSide == List(expr1,t,expr1) || rl.rightSide == List(expr2,t,Nonterminal("primary2")))
      //          println("Rule: "+rl+" count: "+cnt)
      //      }
      cnt
    }

    //check if there is no unit production
    val unitRules = g.rules.collect { case rl @ Rule(_, List(_: Nonterminal)) => rl }
    if (!unitRules.isEmpty)
      throw new IllegalStateException("There are unit productions: " + unitRules.mkString("\n"))

    //initialize the word counts here
    //the complexity of the procedure is O(size^2 * |G|)    
    for (m <- 1 to size) {
      //compute A[m] for every non-terminal 'A' 
      //using post-order to traverse non-terminals      
      for (i <- 0 until N) {
        val nt = indexToNonterms(i)
        val count = g.nontermToRules(nt).map { rl => ruleCountForSize(rl, m) }.sum
        //add this to the wordCount array
        wordCount(i)(m - 1) = count
      }
    }

    //for debugging
    //print the array for debugging
    /*println("Word Count Array:")
    var i = -1
    val str = wordCount.map { clmn =>
      i += 1
      indexToNonterms(i) + " " + clmn.mkString(" ")
    }.mkString("\n")
    println(str)*/
    //System.exit(0)

    /**
     * procedures for looking up bounds
     */
    def boundForNonterminal(nt: Nonterminal, m: Int): BigInt = {
      require(m <= size)
      val index = nontermToIndex(nt)
      wordCount(index)(m - 1)
    }

    /**
     * The rules are sorted by bounds.
     * Using a cache for efficiency. The cache in some sense is an
     * expanded version of the grammar. However, the cache could be cleaned up
     * or can store only the recently used ones.
     */
    var ntrulesCache = Map[(Nonterminal, Int), List[(Rule[T], BigInt)]]()
    def rulesForNonterminal(nt: Nonterminal, m: Int) = {

      if (ntrulesCache.contains((nt, m)))
        ntrulesCache((nt, m))
      else {
        val rulesWithWords = g.nontermToRules(nt).map { rl =>
          (rl, ruleCountForSize(rl, m))
        }.filter(_._2 > 0)

        val rules = rulesWithWords.sortBy(_._2)
        ntrulesCache += ((nt, m) -> rules)
        rules
      }
    }
  }  

  def firstMismatchSize[T](g1: Grammar[T], g2: Grammar[T], maxSize: Int): Option[Int] = {
    val wc1 = new WordCounter(g1, maxSize)
    val wc2 = new WordCounter(g2, maxSize)
    var foundSize: Option[Int] = None

    for (size <- 1 to maxSize) if (!foundSize.isDefined) {
      val dsize1 = wc1.boundForNonterminal(g1.start, size)
      val dsize2 = wc2.boundForNonterminal(g2.start, size)
      if (dsize1 != dsize2) {
        foundSize = Some(size)
      }
    }
    foundSize
  }
}